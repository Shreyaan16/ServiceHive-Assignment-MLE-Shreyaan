{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a56be62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Loading data/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2191/2191 [00:00<00:00, 5121.64it/s]\n",
      "100%|██████████| 2271/2271 [00:00<00:00, 4688.95it/s]\n",
      "100%|██████████| 2404/2404 [00:00<00:00, 5411.67it/s]\n",
      "100%|██████████| 2512/2512 [00:00<00:00, 5671.29it/s]\n",
      "100%|██████████| 2274/2274 [00:00<00:00, 5596.29it/s]\n",
      "100%|██████████| 2382/2382 [00:00<00:00, 5002.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 437/437 [00:00<00:00, 4920.37it/s]\n",
      "100%|██████████| 474/474 [00:00<00:00, 4561.68it/s]\n",
      "100%|██████████| 553/553 [00:00<00:00, 5324.15it/s]\n",
      "100%|██████████| 525/525 [00:00<00:00, 4990.65it/s]\n",
      "100%|██████████| 510/510 [00:00<00:00, 5391.01it/s]\n",
      "100%|██████████| 501/501 [00:00<00:00, 4504.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 14034\n",
      "Number of testing examples: 3000\n",
      "Each image is of size: (150, 150)\n",
      "Preparing data loaders...\n",
      "Starting PSO optimization...\n",
      "PSO Iteration 1/2\n",
      "  Evaluating particle 1/10 - LR: 0.009245, Filters: 30\n",
      "    Accuracy: 0.5868\n",
      "  Evaluating particle 2/10 - LR: 0.009022, Filters: 51\n",
      "    Accuracy: 0.6853\n",
      "  Evaluating particle 3/10 - LR: 0.006173, Filters: 40\n",
      "    Accuracy: 0.6893\n",
      "  Evaluating particle 4/10 - LR: 0.005334, Filters: 56\n",
      "    Accuracy: 0.7446\n",
      "  Evaluating particle 5/10 - LR: 0.004039, Filters: 27\n",
      "    Accuracy: 0.7390\n",
      "  Evaluating particle 6/10 - LR: 0.008859, Filters: 52\n",
      "    Accuracy: 0.6637\n",
      "  Evaluating particle 7/10 - LR: 0.005945, Filters: 61\n",
      "    Accuracy: 0.7319\n",
      "  Evaluating particle 8/10 - LR: 0.007329, Filters: 58\n",
      "    Accuracy: 0.6896\n",
      "  Evaluating particle 9/10 - LR: 0.008836, Filters: 38\n",
      "    Accuracy: 0.1741\n",
      "  Evaluating particle 10/10 - LR: 0.003843, Filters: 28\n",
      "    Accuracy: 0.7852\n",
      "PSO Iteration 2/2\n",
      "  Evaluating particle 1/10 - LR: 0.004030, Filters: 30\n",
      "    Accuracy: 0.7190\n",
      "  Evaluating particle 2/10 - LR: 0.001654, Filters: 47\n",
      "    Accuracy: 0.8219\n",
      "  Evaluating particle 3/10 - LR: 0.004104, Filters: 23\n",
      "    Accuracy: 0.1790\n",
      "  Evaluating particle 4/10 - LR: 0.003441, Filters: 33\n",
      "    Accuracy: 0.7689\n",
      "  Evaluating particle 5/10 - LR: 0.003965, Filters: 28\n",
      "    Accuracy: 0.5983\n",
      "  Evaluating particle 6/10 - LR: 0.000100, Filters: 16\n",
      "    Accuracy: 0.6452\n",
      "  Evaluating particle 7/10 - LR: 0.005663, Filters: 16\n",
      "    Accuracy: 0.7451\n",
      "  Evaluating particle 8/10 - LR: 0.002699, Filters: 58\n",
      "    Accuracy: 0.7802\n",
      "  Evaluating particle 9/10 - LR: 0.008561, Filters: 30\n",
      "    Accuracy: 0.6538\n",
      "  Evaluating particle 10/10 - LR: 0.003843, Filters: 28\n",
      "    Accuracy: 0.7666\n",
      "\n",
      "==================================================\n",
      "PSO Optimization Results:\n",
      "==================================================\n",
      "Best Learning Rate: 0.001654\n",
      "Best Filters: 47\n",
      "Best Training Accuracy: 0.8219\n",
      "==================================================\n",
      "\n",
      "Training final model with optimized parameters...\n",
      "Final model - LR: 0.001654, Filters: 47\n",
      "Epoch [1/20], Loss: 1.2289, Accuracy: 0.5026\n",
      "Epoch [2/20], Loss: 0.9972, Accuracy: 0.5942\n",
      "Epoch [3/20], Loss: 0.9030, Accuracy: 0.6435\n",
      "Epoch [4/20], Loss: 0.8057, Accuracy: 0.6995\n",
      "Epoch [5/20], Loss: 0.7092, Accuracy: 0.7380\n",
      "Epoch [6/20], Loss: 0.6588, Accuracy: 0.7631\n",
      "Epoch [7/20], Loss: 0.6210, Accuracy: 0.7804\n",
      "Epoch [8/20], Loss: 0.5745, Accuracy: 0.7934\n",
      "Epoch [9/20], Loss: 0.5273, Accuracy: 0.8152\n",
      "Epoch [10/20], Loss: 0.5083, Accuracy: 0.8186\n",
      "Epoch [11/20], Loss: 0.4766, Accuracy: 0.8326\n",
      "Epoch [12/20], Loss: 0.4603, Accuracy: 0.8366\n",
      "Epoch [13/20], Loss: 0.4315, Accuracy: 0.8446\n",
      "Epoch [14/20], Loss: 0.4118, Accuracy: 0.8509\n",
      "Epoch [15/20], Loss: 0.3721, Accuracy: 0.8640\n",
      "Epoch [16/20], Loss: 0.3630, Accuracy: 0.8693\n",
      "Epoch [17/20], Loss: 0.3537, Accuracy: 0.8733\n",
      "Epoch [18/20], Loss: 0.3256, Accuracy: 0.8808\n",
      "Epoch [19/20], Loss: 0.3150, Accuracy: 0.8823\n",
      "Epoch [20/20], Loss: 0.3023, Accuracy: 0.8870\n",
      "Evaluating on test set...\n",
      "Final Test Accuracy: 0.8253\n",
      "Model saved as 'best_model_pso.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Configuration\n",
    "class_names = ['buildings','forest','glacier','mountain', 'sea','street']\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "nb_classes = len(class_names)\n",
    "IMAGE_SIZE = (150, 150)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data Loading Function\n",
    "def load_data():\n",
    "    datasets = [\"data/train\", \"data/test\"]\n",
    "    output = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        images = []\n",
    "        labels = []\n",
    "        print(f\"Loading {dataset}\")\n",
    "\n",
    "        for folder in os.listdir(dataset):\n",
    "            label = class_names_label[folder]\n",
    "\n",
    "            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n",
    "                # Get the path name of the image\n",
    "                img_path = os.path.join(os.path.join(dataset, folder), file)\n",
    "                \n",
    "                # Open and resize the img\n",
    "                image = cv2.imread(img_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, IMAGE_SIZE) \n",
    "        \n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "        images = np.array(images, dtype='float32')\n",
    "        labels = np.array(labels, dtype='int32')   \n",
    "        \n",
    "        output.append((images, labels))\n",
    "\n",
    "    return output\n",
    "\n",
    "# CNN Model Definition\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_filters=32):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, num_filters, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 2, kernel_size=3, padding=0)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate size after convolutions: 150->148->74->72->36->34->17->15->7\n",
    "        self.fc1 = nn.Linear(num_filters * 2 * 7 * 7, 64)\n",
    "        self.fc2 = nn.Linear(64, 6)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # No softmax here, CrossEntropyLoss handles it\n",
    "        \n",
    "        return x\n",
    "\n",
    "def create_model(learning_rate=0.001, num_filters=32):\n",
    "    model = CNNModel(num_filters=num_filters)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_data(train_images, train_labels, test_images, test_labels, batch_size=64):\n",
    "    # Convert from (N, H, W, C) to (N, C, H, W) for PyTorch\n",
    "    train_images = np.transpose(train_images, (0, 3, 1, 2))\n",
    "    test_images = np.transpose(test_images, (0, 3, 1, 2))\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_images = torch.FloatTensor(train_images)\n",
    "    train_labels = torch.LongTensor(train_labels)\n",
    "    test_images = torch.FloatTensor(test_images)\n",
    "    test_labels = torch.LongTensor(test_labels)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(train_images, train_labels)\n",
    "    test_dataset = TensorDataset(test_images, test_labels)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, criterion, train_dataloader, epochs=10, verbose=True):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_images, batch_labels in train_dataloader:\n",
    "            batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_images)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += batch_labels.size(0)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy = total_correct / total_samples\n",
    "        epoch_loss = total_loss / len(train_dataloader)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in test_dataloader:\n",
    "            batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += batch_labels.size(0)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Fitness function for PSO\n",
    "def fitness_function(params, train_dataloader):\n",
    "    lr = params[0]\n",
    "    filters = int(params[1])\n",
    "    \n",
    "    model, optimizer, criterion = create_model(learning_rate=lr, num_filters=filters)\n",
    "    accuracies = train_model(model, optimizer, criterion, train_dataloader, epochs=10, verbose=False)\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model, optimizer, criterion\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Return the final accuracy (last epoch)\n",
    "    return accuracies[-1]\n",
    "\n",
    "# PSO Algorithm\n",
    "def PSO(bounds, train_dataloader, n_particles=10, max_iter=2):\n",
    "    dim = len(bounds)\n",
    "    particles = [np.array([random.uniform(low, high) for low, high in bounds]) for _ in range(n_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(n_particles)]\n",
    "    personal_best = particles.copy()\n",
    "    personal_best_scores = [0] * n_particles\n",
    "    global_best = None\n",
    "    global_best_score = 0\n",
    "\n",
    "    w = 0.9  # inertia weight\n",
    "    c1 = 2.0  # cognitive parameter\n",
    "    c2 = 2.0  # social parameter\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        print(f\"PSO Iteration {it+1}/{max_iter}\")\n",
    "        \n",
    "        for i in range(n_particles):\n",
    "            print(f\"  Evaluating particle {i+1}/{n_particles} - LR: {particles[i][0]:.6f}, Filters: {int(particles[i][1])}\")\n",
    "            score = fitness_function(particles[i], train_dataloader)\n",
    "            print(f\"    Accuracy: {score:.4f}\")\n",
    "            \n",
    "            if score > personal_best_scores[i]:\n",
    "                personal_best_scores[i] = score\n",
    "                personal_best[i] = particles[i].copy()\n",
    "            if score > global_best_score:\n",
    "                global_best_score = score\n",
    "                global_best = particles[i].copy()\n",
    "\n",
    "        # Update particle velocities and positions\n",
    "        for i in range(n_particles):\n",
    "            r1, r2 = np.random.rand(dim), np.random.rand(dim)\n",
    "            cognitive = c1 * r1 * (personal_best[i] - particles[i])\n",
    "            social = c2 * r2 * (global_best - particles[i])\n",
    "            velocities[i] = w * velocities[i] + cognitive + social\n",
    "            particles[i] += velocities[i]\n",
    "\n",
    "            # Apply bounds constraints\n",
    "            for d in range(dim):\n",
    "                low, high = bounds[d]\n",
    "                particles[i][d] = np.clip(particles[i][d], low, high)\n",
    "        \n",
    "        # Decay inertia weight\n",
    "        w *= 0.95\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PSO Optimization Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Best Learning Rate: {global_best[0]:.6f}\")\n",
    "    print(f\"Best Filters: {int(global_best[1])}\")\n",
    "    print(f\"Best Training Accuracy: {global_best_score:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return global_best, global_best_score\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    print(\"Loading data...\")\n",
    "    (train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "    \n",
    "    # Shuffle training data\n",
    "    train_images, train_labels = shuffle(train_images, train_labels, random_state=25)\n",
    "    \n",
    "    # Normalize images\n",
    "    train_images = train_images / 255.0 \n",
    "    test_images = test_images / 255.0\n",
    "    \n",
    "    n_train = train_labels.shape[0]\n",
    "    n_test = test_labels.shape[0]\n",
    "    \n",
    "    print(f\"Number of training examples: {n_train}\")\n",
    "    print(f\"Number of testing examples: {n_test}\")\n",
    "    print(f\"Each image is of size: {IMAGE_SIZE}\")\n",
    "    \n",
    "    # Prepare dataloaders\n",
    "    print(\"Preparing data loaders...\")\n",
    "    train_dataloader, test_dataloader = prepare_data(train_images, train_labels, \n",
    "                                                   test_images, test_labels, batch_size=64)\n",
    "    \n",
    "    # Define PSO bounds [learning_rate, num_filters]\n",
    "    bounds = [(0.0001, 0.01), (16, 64)]\n",
    "    \n",
    "    # Run PSO optimization\n",
    "    print(\"Starting PSO optimization...\")\n",
    "    best_params, best_score = PSO(bounds=bounds, train_dataloader=train_dataloader, \n",
    "                                 n_particles=10, max_iter=2)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model with optimized parameters...\")\n",
    "    final_model, final_optimizer, final_criterion = create_model(\n",
    "        learning_rate=best_params[0], \n",
    "        num_filters=int(best_params[1])\n",
    "    )\n",
    "    \n",
    "    print(f\"Final model - LR: {best_params[0]:.6f}, Filters: {int(best_params[1])}\")\n",
    "    final_accuracies = train_model(final_model, final_optimizer, final_criterion, \n",
    "                                 train_dataloader, epochs=20, verbose=True)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_accuracy = evaluate_model(final_model, test_dataloader)\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save({\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': final_optimizer.state_dict(),\n",
    "        'learning_rate': best_params[0],\n",
    "        'num_filters': int(best_params[1]),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'class_names': class_names\n",
    "    }, 'best_model_pso.pth')\n",
    "    \n",
    "    print(\"Model saved as 'best_model_pso.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f398da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path='best_model_pso.pth'):\n",
    "    \"\"\"Load a trained model from file\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file '{model_path}' not found. Make sure you've trained and saved the model first.\")\n",
    "    \n",
    "    try:\n",
    "        # Fix for PyTorch 2.6+ - set weights_only=False to allow loading numpy objects\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        \n",
    "        # Create model with same parameters used during training\n",
    "        model = CNNModel(num_filters=checkpoint['num_filters'])\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(f\"Model loaded successfully!\")\n",
    "        print(f\"- Filters: {checkpoint['num_filters']}\")\n",
    "        print(f\"- Learning Rate: {checkpoint['learning_rate']}\")\n",
    "        print(f\"- Test Accuracy: {checkpoint['test_accuracy']:.4f}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def predict_single_image(model, image_path):\n",
    "    \"\"\"Predict class for a single image\"\"\"\n",
    "    try:\n",
    "        # Preprocess image\n",
    "        image_tensor = preprocess_image(image_path)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class_idx].item()\n",
    "        \n",
    "        predicted_class = class_names[predicted_class_idx]\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting image {image_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Preprocess a single image for prediction\"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not read image from {image_path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize to model input size\n",
    "    image = cv2.resize(image, IMAGE_SIZE)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    image = image.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert from (H, W, C) to (C, H, W)\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension: (C, H, W) -> (1, C, H, W)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.FloatTensor(image).to(device)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_with_all_probabilities(model, image_path):\n",
    "    \"\"\"Predict class and return all class probabilities\"\"\"\n",
    "    try:\n",
    "        image_tensor = preprocess_image(image_path)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {}\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            results[class_name] = probabilities[i]\n",
    "        \n",
    "        # Sort by probability (highest first)\n",
    "        sorted_results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        return sorted_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting image {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a60e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  predicted_class, confidence, all_probs = predict_single_image(final_model, \"data/pred/24144.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f7b815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glacier'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc06a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: best_model_pso.pth\n",
      "Model loaded successfully!\n",
      "- Filters: 47\n",
      "- Learning Rate: 0.0016538263873647147\n",
      "- Test Accuracy: 0.8253\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model(\"best_model_pso.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861984c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
